{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_data = loadarff('breast.w.arff')\n",
    "df_data = pd.DataFrame(raw_data[0]).dropna()  # converting data to a pandas DataFrame\n",
    "df_data['Class'].replace({b'malignant': 1, b'benign': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix of the a MLP with two hidden layers of sizes 3 and 2 in the presence and absence of early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = df_data.drop(columns='Class').values, df_data['Class'].values\n",
    "\n",
    "predicted_targets1, predicted_targets2, actual_targets = np.array([]), np.array([]), np.array([])\n",
    "\n",
    "# creating MLP Classifiers with and without early stopping and ajusting max_iter for the model to converge\n",
    "clasf1 = MLPClassifier(hidden_layer_sizes=(3,2,), early_stopping=False, alpha = 0.1, max_iter = 2000)\n",
    "clasf2 = MLPClassifier(hidden_layer_sizes=(3,2,), early_stopping=True, alpha = 0.1)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) # Stratifued CV is better for classification\n",
    "\n",
    "for train_subset, test_subset in kf.split(data, target):\n",
    "   X_train, X_test = data[train_subset], data[test_subset]\n",
    "   Y_train, Y_test = target[train_subset], target[test_subset]\n",
    "\n",
    "   c1 = clasf1.fit(X_train, Y_train)  # train classifier without early stopping\n",
    "   predicted_targets1 = np.append(predicted_targets1, c1.predict(X_test))  # test and store predicted values\n",
    "\n",
    "   c2 = clasf2.fit(X_train, Y_train)  # train classifier with early stopping\n",
    "   predicted_targets2 = np.append(predicted_targets2, c2.predict(X_test))  # test and store predicted values\n",
    "\n",
    "   actual_targets = np.append(actual_targets, Y_test) # store the actual values\n",
    "\n",
    "cnf_matrix1 = confusion_matrix(actual_targets, predicted_targets1, labels=clasf1.classes_)\n",
    "ConfusionMatrixDisplay(cnf_matrix1, display_labels=clasf1.classes_).plot(cmap=plt.get_cmap('PuBuGn'))\n",
    "plt.title(\"Without early stopping\")\n",
    "plt.show()\n",
    "\n",
    "cnf_matrix2 = confusion_matrix(actual_targets, predicted_targets2, labels=clasf2.classes_)\n",
    "ConfusionMatrixDisplay(cnf_matrix2, display_labels=clasf2.classes_).plot(cmap=plt.get_cmap('PuBuGn'))\n",
    "plt.title(\"With early stopping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the distribution of the residues using boxplots in the presence and absence of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = loadarff('kin8nm.arff')\n",
    "df_data = pd.DataFrame(raw_data[0])  # converting data to a pandas DataFrame\n",
    "\n",
    "data, target = df_data.drop(columns='y').values, df_data['y'].values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "residues1, residues2, residues3, residues4 = [], [], [], []\n",
    "\n",
    "# Creating MLP Regressors with different regularization terms and without regularization\n",
    "regr1 = MLPRegressor(hidden_layer_sizes=(3,2,), alpha = 0.1)\n",
    "regr2 = MLPRegressor(hidden_layer_sizes=(3,2,), alpha = 1)\n",
    "regr3 = MLPRegressor(hidden_layer_sizes=(3,2,), alpha = 10)\n",
    "regr4 = MLPRegressor(hidden_layer_sizes=(3,2,), alpha = 0)\n",
    "\n",
    "for train_subset, test_subset in kf.split(data):\n",
    "   X_train, X_test = data[train_subset], data[test_subset]\n",
    "   Y_train, Y_test = target[train_subset], target[test_subset]\n",
    "   # train\n",
    "   regr1.fit(X_train, Y_train)\n",
    "   regr2.fit(X_train, Y_train) \n",
    "   regr3.fit(X_train, Y_train) \n",
    "   regr4.fit(X_train, Y_train)\n",
    "   # store the residue for each prediction: absolute value of (actual - predicted)\n",
    "   residues1.extend(np.absolute(np.subtract(Y_test, regr1.predict(X_test))))\n",
    "   residues2.extend(np.absolute(np.subtract(Y_test, regr2.predict(X_test))))\n",
    "   residues3.extend(np.absolute(np.subtract(Y_test, regr3.predict(X_test))))\n",
    "   residues4.extend(np.absolute(np.subtract(Y_test, regr4.predict(X_test))))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Residues Distribution\")\n",
    "ax.boxplot([residues1, residues2, residues3, residues4], labels=[\"alpha 0.1\", \"alpha 1\", \"alpha 10\", \"Without regularization\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
