{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#---------------------------------- Parse of input ----------------------------#\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "raw_data = loadarff('breast.w.arff')\n",
    "df_data = pd.DataFrame(raw_data[0])  # converting data to a pandas DataFrame\n",
    "df_data = df_data.dropna()  # all rows with Na values are dropped\n",
    "df_data['Class'].replace({b'malignant': 1, b'benign': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#-------- Train and test of a decision tree classifier varying #features and max-depth ---------#\n",
    "data, target = df_data.drop(columns='Class'), df_data['Class']\n",
    "tra_acc_features, test_acc_features, tra_acc_depth, test_acc_depth = [\n",
    "], [], [], []  # to save accuries to be plotted\n",
    "# number of the features and maximum depth to be looped through\n",
    "values = [1, 3, 5, 9]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.3, random_state=10)\n",
    "for v in values:\n",
    "    # select k best features using MI value\n",
    "    kbest = SelectKBest(mutual_info_classif, k=v)\n",
    "    kbest.fit(data, target)\n",
    "    # get the names of the best k features\n",
    "    cols = kbest.get_support(indices=True)\n",
    "    # gets only the columns of the featured selected\n",
    "    x_train_features, x_test_features = x_train.iloc[:,\n",
    "                                                     cols], x_test.iloc[:, cols]\n",
    "    clf_features = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    clf_features.fit(x_train_features, y_train)  # train max_features tree\n",
    "    # max_depth is defined as parameter\n",
    "    clf_depth = DecisionTreeClassifier(criterion=\"entropy\", max_depth=v)\n",
    "    clf_depth.fit(x_train, y_train)  # train max_depth tree\n",
    "    tra_acc_features.append(clf_features.score(\n",
    "        x_train_features, y_train))  # test on train set\n",
    "    test_acc_features.append(clf_features.score(\n",
    "        x_test_features, y_test))  # test on test set\n",
    "    tra_acc_depth.append(clf_depth.score(x_train, y_train))\n",
    "    test_acc_depth.append(clf_depth.score(x_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(values, tra_acc_features, 'co:', label=\"X features training set\")\n",
    "plt.plot(values, test_acc_features, 'darkcyan', marker='D',\n",
    "         linestyle=\":\", label=\"X features testing set\")\n",
    "plt.plot(values, tra_acc_depth, 'ro:', label=\"Maximum depth = X training set\")\n",
    "plt.plot(values, test_acc_depth, color=\"firebrick\", marker='D',\n",
    "         linestyle=\":\", label=\"Maximum depth = X testing set\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('plots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Search with CV to find the best max-depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [1, 3, 5, 9]}\n",
    "# decides which is the best hyperparameter for the decision tree\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), parameters)\n",
    "clf.fit(data, target)\n",
    "print(\"Best score\", clf.best_score_, \" Best depth\", clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
